{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://skills.network\" target=\"_blank\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\">\n",
    "    </a>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice Project\n",
    "\n",
    "Estimated time needed: **60** minutes\n",
    "\n",
    "Ce projet pratique se concentre sur la transformation et l'intégration des données à l'aide de PySpark. Vous travaillerez avec deux ensembles de données, effectuerez diverses transformations telles que l'ajout de colonnes, le renommage de colonnes, la suppression de colonnes inutiles, la jointure de DataFrames, et enfin, vous écrirez les résultats à la fois dans un entrepôt Hive et un système de fichiers HDFS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prérequis\n",
    "Pour cet exercice, vous utiliserez wget, Python et Spark (PySpark). Il est donc essentiel de vous assurer que les bibliothèques spécifiées ci-dessous sont installées dans votre environnement de travail ou au sein des Labs de Skills Network (SN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wget\n",
      "  Downloading wget-3.2.zip (10 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: pyspark in /home/tegongue/anaconda3/lib/python3.12/site-packages (3.5.3)\n",
      "Requirement already satisfied: findspark in /home/tegongue/anaconda3/lib/python3.12/site-packages (2.0.1)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /home/tegongue/anaconda3/lib/python3.12/site-packages (from pyspark) (0.10.9.7)\n",
      "Building wheels for collected packages: wget\n",
      "  Building wheel for wget (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9656 sha256=562f48c4eaaf33d13cb6fcfc8caf0e5e0cc7272600dd686fdedb86c4a0d2d0ec\n",
      "  Stored in directory: /home/tegongue/.cache/pip/wheels/01/46/3b/e29ffbe4ebe614ff224bad40fc6a5773a67a163251585a13a9\n",
      "Successfully built wget\n",
      "Installing collected packages: wget\n",
      "Successfully installed wget-3.2\n"
     ]
    }
   ],
   "source": [
    "# Installing required packages\n",
    "\n",
    "!pip install wget pyspark  findspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prework - Initiate the Spark Session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PySpark is the Spark API for Python. In this lab, we use PySpark to initialize the SparkContext.   \n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/01 21:07:33 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "# Creating a SparkContext object\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "# Creating a Spark Session\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark DataFrames basic example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Load datasets into PySpark DataFrames\n",
    "\n",
    "Download the datasets from the folloing links using `wget` and load it in a Spark Dataframe.\n",
    "\n",
    "1. https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/labs/data/dataset1.csv  \n",
    "2. https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/labs/data/dataset2.csv  \n",
    "\n",
    "*Hint: Import wget*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dataset2.csv'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Télécharger le jeu de données en utilisant wget\n",
    "import wget\n",
    "\n",
    "link_to_data1 = 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/labs/data/dataset1.csv'\n",
    "wget.download(link_to_data1)\n",
    "\n",
    "link_to_data2 = 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/labs/data/dataset2.csv'\n",
    "wget.download(link_to_data2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<details>\n",
    "    <summary>Click here for Solution</summary>\n",
    "\n",
    "```python\n",
    "# download the dataset using wget\n",
    "import wget\n",
    "\n",
    "link_to_data1 = 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/labs/data/dataset1.csv'\n",
    "wget.download(link_to_data1)\n",
    "\n",
    "link_to_data2 = 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/labs/data/dataset2.csv'\n",
    "wget.download(link_to_data2)\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+------+-----------+--------+\n",
      "|customer_id|date_column|amount|description|location|\n",
      "+-----------+-----------+------+-----------+--------+\n",
      "|          1|   1/1/2022|  5000| Purchase A| Store A|\n",
      "|          2|  15/2/2022|  1200| Purchase B| Store B|\n",
      "|          3|  20/3/2022|   800| Purchase C| Store C|\n",
      "|          4|  10/4/2022|  3000| Purchase D| Store D|\n",
      "|          5|   5/5/2022|  6000| Purchase E| Store E|\n",
      "|          6|  10/6/2022|  4500| Purchase F| Store F|\n",
      "|          7|  15/7/2022|   200| Purchase G| Store G|\n",
      "|          8|  20/8/2022|  3500| Purchase H| Store H|\n",
      "|          9|  25/9/2022|   700| Purchase I| Store I|\n",
      "|         10| 30/10/2022|  1800| Purchase J| Store J|\n",
      "|         11|  5/11/2022|  2200| Purchase K| Store K|\n",
      "|         12| 10/12/2022|   900| Purchase L| Store L|\n",
      "|         13|  15/1/2023|  4800| Purchase M| Store M|\n",
      "|         14|  20/2/2023|   300| Purchase N| Store N|\n",
      "|         15|  25/3/2023|  4200| Purchase O| Store O|\n",
      "|         16|  30/4/2023|  2600| Purchase P| Store P|\n",
      "|         17|   5/5/2023|   700| Purchase Q| Store Q|\n",
      "|         18|  10/6/2023|  1500| Purchase R| Store R|\n",
      "|         19|  15/7/2023|  3200| Purchase S| Store S|\n",
      "|         20|  20/8/2023|  1000| Purchase T| Store T|\n",
      "+-----------+-----------+------+-----------+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-----------+----------------+-----+-------+\n",
      "|customer_id|transaction_date|value|  notes|\n",
      "+-----------+----------------+-----+-------+\n",
      "|          1|        1/1/2022| 1500| Note 1|\n",
      "|          2|       15/2/2022| 2000| Note 2|\n",
      "|          3|       20/3/2022| 1000| Note 3|\n",
      "|          4|       10/4/2022| 2500| Note 4|\n",
      "|          5|        5/5/2022| 1800| Note 5|\n",
      "|          6|       10/6/2022| 1200| Note 6|\n",
      "|          7|       15/7/2022|  700| Note 7|\n",
      "|          8|       20/8/2022| 3000| Note 8|\n",
      "|          9|       25/9/2022|  600| Note 9|\n",
      "|         10|      30/10/2022| 1200|Note 10|\n",
      "|         11|       5/11/2022| 1500|Note 11|\n",
      "|         12|      10/12/2022|  800|Note 12|\n",
      "|         13|       15/1/2023| 2000|Note 13|\n",
      "|         14|       20/2/2023|  700|Note 14|\n",
      "|         15|       25/3/2023| 1800|Note 15|\n",
      "|         16|       30/4/2023| 1000|Note 16|\n",
      "|         17|        5/5/2023|  400|Note 17|\n",
      "|         18|       10/6/2023| 1500|Note 18|\n",
      "|         19|       15/7/2023| 3000|Note 19|\n",
      "|         20|       20/8/2023|  600|Note 20|\n",
      "+-----------+----------------+-----+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Charger les deux fichiers CSV dans des DataFrames\n",
    "df1 = spark.read.csv(\"dataset1.csv\", header=True, inferSchema=True)\n",
    "df2 = spark.read.csv(\"dataset2.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Afficher les premières lignes des deux DataFrames\n",
    "df1.show()\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<details>\n",
    "    <summary>Click here for Solution</summary>\n",
    "\n",
    "```python\n",
    "\n",
    "#load the data into a pyspark dataframe\n",
    "    \n",
    "df1 = spark.read.csv(\"dataset1.csv\", header=True, inferSchema=True)\n",
    "df2 = spark.read.csv(\"dataset2.csv\", header=True, inferSchema=True)\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Display the schema of both dataframes\n",
    "\n",
    "Display the schema of `df1` and `df2` to understand the structure of the datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- date_column: string (nullable = true)\n",
      " |-- amount: integer (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- transaction_date: string (nullable = true)\n",
      " |-- value: integer (nullable = true)\n",
      " |-- notes: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Afficher le schéma de df1\n",
    "df1.printSchema()\n",
    "\n",
    "# Afficher le schéma de df2\n",
    "df2.printSchema()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<details>\n",
    "    <summary>Click here for Solution</summary>\n",
    "\n",
    "```python\n",
    "\n",
    "#print the schema of df1 and df2\n",
    "    \n",
    "df1.printSchema()\n",
    "df2.printSchema()\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3: Add a new column to each dataframe\n",
    "\n",
    "Add a new column named **year** to `df1` and **quarter** to `df2` representing the year and quarter of the data.\n",
    "\n",
    "*Hint: use withColumn. Convert the date columns which are present as string to date before extracting the year and quarter information*\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.functions import year, quarter\n",
    "\n",
    "#Add new column year to df1\n",
    "df1 = df1.withColumn('year', year(to_date('date_column','dd/MM/yyyy')))\n",
    "    \n",
    "#Add new column quarter to df2    \n",
    "df2 = df2.withColumn('quarter', quarter(to_date('transaction_date','dd/MM/yyyy')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<details>\n",
    "    <summary>Click here for Solution</summary>\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import year, quarter\n",
    "\n",
    "#Add new column year to df1\n",
    "df1 = df1.withColumn('year', year(to_date('date_column','dd/MM/yyyy')))\n",
    "    \n",
    "#Add new column quarter to df2    \n",
    "df2 = df2.withColumn('quarter', quarter(to_date('transaction_date','dd/MM/yyyy')))```\n",
    "\n",
    "</details>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 4: Rename columns in both dataframes\n",
    "\n",
    "Rename the column **amount** to **transaction_amount** in `df1` and **value** to **transaction_value** in `df2`.\n",
    "\n",
    "*Hint: Use withColumnRenamed*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renommer la colonne 'amount' en 'transaction_amount' dans df1\n",
    "df1 = df1.withColumnRenamed('amount', 'transaction_amount')\n",
    "\n",
    "# Renommer la colonne 'value' en 'transaction_value' dans df2\n",
    "df2 = df2.withColumnRenamed('value', 'transaction_value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<details>\n",
    "    <summary>Click here for Solution</summary>\n",
    "\n",
    "```python\n",
    "\n",
    "#Rename df1 column amount to transaction_amount\n",
    "df1 = df1.withColumnRenamed('amount', 'transaction_amount')\n",
    "    \n",
    "#Rename df2 column value to transaction_value\n",
    "df2 = df2.withColumnRenamed('value', 'transaction_value')\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 5: Drop unnecessary columns\n",
    "\n",
    "Drop the columns **description** and **location** from `df1` and **notes** from `df2`.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supprimer les colonnes 'description' et 'location' de df1\n",
    "df1 = df1.drop('description', 'location')\n",
    "\n",
    "# Supprimer la colonne 'notes' de df2\n",
    "df2 = df2.drop('notes')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "<details>\n",
    "    <summary>Click here for Solution</summary>\n",
    "\n",
    "```python\n",
    "\n",
    "#Drop columns description and location from df1\n",
    "df1 = df1.drop('description', 'location')\n",
    "    \n",
    "#Drop column notes from df2\n",
    "df2 = df2.drop('notes')\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 6: Join dataframes based on a common column\n",
    "\n",
    "Join `df1` and `df2` based on the common column **customer_id** and create a new dataframe named `joined_df`.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#join df1 and df2 based on common column customer_id\n",
    "joined_df = df1.join(df2, 'customer_id', 'inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for Solution</summary>\n",
    "\n",
    "```python\n",
    "\n",
    "#join df1 and df2 based on common column customer_id\n",
    "joined_df = df1.join(df2, 'customer_id', 'inner')\n",
    "    \n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 7: Filter data based on a condition\n",
    "\n",
    "Filter `joined_df` to include only transactions where \"transaction_amount\" is greater than 1000 and create a new dataframe named `filtered_df`.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrer les transactions où 'transaction_amount' est supérieur à 1000\n",
    "filtered_df = joined_df.filter(\"transaction_amount > 1000\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for Solution</summary>\n",
    "\n",
    "```python\n",
    "\n",
    "# filter the dataframe for transaction amount > 1000\n",
    "filtered_df = joined_df.filter(\"transaction_amount > 1000\")    \n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 8: Aggregate data by customer\n",
    "\n",
    "Calculate the total transaction amount for each customer in `filtered_df` and display the result.\n",
    "\n",
    "*Hint: Use sum from pyspark.sql.functions*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------------+\n",
      "|customer_id|total_transaction_amount|\n",
      "+-----------+------------------------+\n",
      "|         31|                    3200|\n",
      "|         85|                    1800|\n",
      "|         78|                    1500|\n",
      "|         34|                    1200|\n",
      "|         81|                    5500|\n",
      "|         28|                    2600|\n",
      "|         76|                    2600|\n",
      "|         27|                    4200|\n",
      "|         91|                    3200|\n",
      "|         22|                    1200|\n",
      "|         93|                    5500|\n",
      "|          1|                    5000|\n",
      "|         52|                    2600|\n",
      "|         13|                    4800|\n",
      "|          6|                    4500|\n",
      "|         16|                    2600|\n",
      "|         40|                    2600|\n",
      "|         94|                    1200|\n",
      "|         57|                    5500|\n",
      "|         54|                    1500|\n",
      "|         96|                    2400|\n",
      "|         48|                    2400|\n",
      "|          5|                    6000|\n",
      "|         19|                    3200|\n",
      "|         64|                    2600|\n",
      "|         15|                    4200|\n",
      "|         43|                    3200|\n",
      "|         37|                    1800|\n",
      "|         61|                    1800|\n",
      "|         88|                    2600|\n",
      "|         72|                    2400|\n",
      "|          4|                    3000|\n",
      "|         55|                    3200|\n",
      "|          8|                    3500|\n",
      "|        100|                    2600|\n",
      "|         39|                    4200|\n",
      "|         49|                    1800|\n",
      "|         84|                    2400|\n",
      "|         87|                    4200|\n",
      "|         51|                    4200|\n",
      "|         69|                    5500|\n",
      "|         97|                    1800|\n",
      "|         63|                    4200|\n",
      "|         10|                    1800|\n",
      "|         45|                    5500|\n",
      "|         82|                    1200|\n",
      "|         25|                    1800|\n",
      "|         73|                    1800|\n",
      "|         24|                    2400|\n",
      "|         70|                    1200|\n",
      "+-----------+------------------------+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# group by customer_id and aggregate the sum of transaction amount\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Agrégation des données par client et calcul du montant total des transactions\n",
    "aggregated_df = filtered_df.groupBy('customer_id').agg(\n",
    "    F.sum('transaction_amount').alias('total_transaction_amount')\n",
    ")\n",
    "\n",
    "# Affichage du résultat\n",
    "aggregated_df.show(50)\n",
    "\n",
    "\n",
    "#display the result\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for Solution</summary>\n",
    "\n",
    "```python\n",
    "\n",
    "from pyspark.sql.functions import sum\n",
    "   \n",
    "# group by customer_id and aggregate the sum of transaction amount\n",
    "\n",
    "total_amount_per_customer = filtered_df.groupBy('customer_id').agg(sum('transaction_amount').alias('total_amount'))\n",
    "\n",
    "#display the result\n",
    "total_amount_per_customer.show()\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 9: Write the result to a Hive table\n",
    "\n",
    "Write `total_amount_per_customer` to a Hive table named **customer_totals**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been written to Hive table 'customer_totals'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Créez ou remplacez la table Hive customer_totals\n",
    "aggregated_df.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"customer_totals\")\n",
    "\n",
    "print(\"Data has been written to Hive table 'customer_totals'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<details>\n",
    "    <summary>Click here for Solution</summary>\n",
    "\n",
    "```python\n",
    "\n",
    "# Write total_amount_per_customer to a Hive table named customer_totals\n",
    "total_amount_per_customer.write.mode(\"overwrite\").saveAsTable(\"customer_totals\")\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 10: Write the filtered data to HDFS\n",
    "\n",
    "Write `filtered_df` to HDFS in parquet format to a file named **filtered_data**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.write.mode(\"overwrite\").parquet(\"filtered_data.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<details>\n",
    "    <summary>Click here for Solution</summary>\n",
    "\n",
    "```python\n",
    "\n",
    "#Write filtered_df to HDFS in parquet format file filtered_data\n",
    "\n",
    "filtered_df.write.mode(\"overwrite\").parquet(\"filtered_data.parquet\")\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 11: Add a new column based on a condition\n",
    "\n",
    "Ajouter une nouvelle colonne nommée high_value au DataFrame df1 indiquant si le transaction_amount est supérieur à 5000. Lorsque la valeur est supérieure à 5000, la valeur de la colonne doit être \"Yes\". Lorsque la valeur est inférieure ou égale à 5000, la valeur de la colonne doit être \"No\".\n",
    "\n",
    "Indice : Utilisez les fonctions when et lit de pyspark.sql.functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add new column with value indicating whether transaction amount is > 5000 or not\n",
    "\n",
    "from pyspark.sql.functions import when, lit\n",
    "\n",
    "# Add new column with value indicating whether transaction amount is > 5000 or not\n",
    "df1 = df1.withColumn(\"high_value\", when(df1.transaction_amount > 5000, lit(\"Yes\")).otherwise(lit(\"No\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<details>\n",
    "    <summary>Click here for Solution</summary>\n",
    "\n",
    "```python\n",
    "\n",
    "from pyspark.sql.functions import when, lit\n",
    "\n",
    "# Add new column with value indicating whether transaction amount is > 5000 or not\n",
    "df1 = df1.withColumn(\"high_value\", when(df1.transaction_amount > 5000, lit(\"Yes\")).otherwise(lit(\"No\")))\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 12: Calculate the average transaction value per quarter\n",
    "\n",
    "Calculate and display the average transaction value for each quarter in `df2` and create a new dataframe named `average_value_per_quarter` with column `avg_trans_val`.\n",
    "\n",
    "*Hint: Use avg from pyspark.sql.functions*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "\n",
    "#calculate the average transaction value for each quarter in df2\n",
    "average_value_per_quarter = df2.groupBy('quarter').agg(avg(\"transaction_value\").alias(\"avg_trans_val\"))\n",
    "\n",
    "    \n",
    "# #show the average transaction value for each quarter in df2    \n",
    "# average_value_per_quarter.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<details>\n",
    "    <summary>Click here for Solution</summary>\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import avg\n",
    "\n",
    "#calculate the average transaction value for each quarter in df2\n",
    "average_value_per_quarter = df2.groupBy('quarter').agg(avg(\"transaction_value\").alias(\"avg_trans_val\"))\n",
    "\n",
    "    \n",
    "#show the average transaction value for each quarter in df2    \n",
    "average_value_per_quarter.show()\n",
    "\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 13: Write the result to a Hive table\n",
    "\n",
    "Write `average_value_per_quarter` to a Hive table named **quarterly_averages**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/01 21:57:39 ERROR Executor: Exception in task 0.0 in stage 67.0 (TID 62)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_97602/2650978482.py\", line 9, in normalize_date\n",
      "TypeError: strptime() argument 1 must be str, not None\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "24/12/01 21:57:39 WARN TaskSetManager: Lost task 0.0 in stage 67.0 (TID 62) (tegongue-Latitude-5580.lan executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_97602/2650978482.py\", line 9, in normalize_date\n",
      "TypeError: strptime() argument 1 must be str, not None\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "\n",
      "24/12/01 21:57:39 ERROR TaskSetManager: Task 0 in stage 67.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/tmp/ipykernel_97602/2650978482.py\", line 9, in normalize_date\nTypeError: strptime() argument 1 must be str, not None\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[192], line 24\u001b[0m\n\u001b[1;32m     15\u001b[0m df2 \u001b[38;5;241m=\u001b[39m df2\u001b[38;5;241m.\u001b[39mwithColumn(\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransaction_date\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     17\u001b[0m     F\u001b[38;5;241m.\u001b[39mto_date(col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransaction_date\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myyyy-MM-dd\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     18\u001b[0m )\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Ensuite, vous pouvez appliquer votre transformation en toute sécurité\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Exemple d'écriture dans une table Hive\u001b[39;00m\n\u001b[1;32m     22\u001b[0m average_value_per_quarter\u001b[38;5;241m.\u001b[39mwrite \\\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;241m.\u001b[39mmode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[0;32m---> 24\u001b[0m     \u001b[38;5;241m.\u001b[39msaveAsTable(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquarterly_averages\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pyspark/sql/readwriter.py:1586\u001b[0m, in \u001b[0;36mDataFrameWriter.saveAsTable\u001b[0;34m(self, name, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m   1584\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1585\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mformat\u001b[39m)\n\u001b[0;32m-> 1586\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39msaveAsTable(name)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/tmp/ipykernel_97602/2650978482.py\", line 9, in normalize_date\nTypeError: strptime() argument 1 must be str, not None\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Remplacer les valeurs manquantes par une chaîne vide\n",
    "df2 = df2.fillna({'transaction_date': ''})\n",
    "\n",
    "# Ajouter une vérification supplémentaire avant la conversion de la date\n",
    "df2 = df2.withColumn(\n",
    "    \"transaction_date\",\n",
    "    when(col(\"transaction_date\").isNull() | (col(\"transaction_date\") == ''), '1970-01-01')  # Mettre une valeur par défaut\n",
    "    .otherwise(col(\"transaction_date\"))\n",
    ")\n",
    "\n",
    "# Si vous appliquez une fonction de conversion de date, assurez-vous de bien traiter les valeurs\n",
    "df2 = df2.withColumn(\n",
    "    \"transaction_date\",\n",
    "    F.to_date(col(\"transaction_date\"), \"yyyy-MM-dd\")\n",
    ")\n",
    "\n",
    "# Ensuite, vous pouvez appliquer votre transformation en toute sécurité\n",
    "# Exemple d'écriture dans une table Hive\n",
    "average_value_per_quarter.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"quarterly_averages\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<details>\n",
    "    <summary>Click here for Solution</summary>\n",
    "\n",
    "```python\n",
    "\n",
    "#Write average_value_per_quarter to a Hive table named quarterly_averages\n",
    "\n",
    "average_value_per_quarter.write.mode(\"overwrite\").saveAsTable(\"quarterly_averages\")\n",
    "\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 14: Calculate the total transaction value per year\n",
    "\n",
    "Calculate and display the total transaction value for each year in `df1` and create a new dataframe named `total_value_per_year` with column `total_transaction_val`.\n",
    "> **Note:** The provided DataFrame `df1` does not explicitly have a `year` column initially. However, in Task 3, a new column named `year` is added to `df1` by extracting the year from the date column. Additionally, in Task 4, the column `amount` is renamed to `transaction_amount`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'int' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[194], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# calculate the total transaction value for each year in df1.\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m total_value_per_year \u001b[38;5;241m=\u001b[39m df1\u001b[38;5;241m.\u001b[39mgroupBy(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myear\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39magg(\u001b[38;5;28msum\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransaction_amount\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal_transaction_val\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# show the total transaction value for each year in df1.\u001b[39;00m\n\u001b[1;32m      5\u001b[0m total_value_per_year\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'int' and 'str'"
     ]
    }
   ],
   "source": [
    "# calculate the total transaction value for each year in df1.\n",
    "total_value_per_year = df1.groupBy('year').agg(sum(\"transaction_amount\").alias(\"total_transaction_val\"))\n",
    "\n",
    "# show the total transaction value for each year in df1.\n",
    "total_value_per_year.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<details>\n",
    "    <summary>Click here for Solution</summary>\n",
    "\n",
    "```python\n",
    "\n",
    "# calculate the total transaction value for each year in df1.\n",
    "total_value_per_year = df1.groupBy('year').agg(sum(\"transaction_amount\").alias(\"total_transaction_val\"))\n",
    "\n",
    "# show the total transaction value for each year in df1.\n",
    "total_value_per_year.show()\n",
    "\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 15: Write the result to HDFS\n",
    "\n",
    "Write `total_value_per_year` to HDFS in the CSV format to file named **total_value_per_year**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'total_value_per_year' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[196], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Write total_value_per_year to HDFS in the CSV format\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#Write total_value_per_year to HDFS in the CSV format\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m total_value_per_year\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mmode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mcsv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal_value_per_year.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'total_value_per_year' is not defined"
     ]
    }
   ],
   "source": [
    "#Write total_value_per_year to HDFS in the CSV format\n",
    "#Write total_value_per_year to HDFS in the CSV format\n",
    "\n",
    "total_value_per_year.write.mode(\"overwrite\").csv(\"total_value_per_year.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<details>\n",
    "    <summary>Click here for Solution</summary>\n",
    "\n",
    "```python\n",
    "\n",
    "#Write total_value_per_year to HDFS in the CSV format\n",
    "\n",
    "total_value_per_year.write.mode(\"overwrite\").csv(\"total_value_per_year.csv\")\n",
    "\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congratulations! You have completed the lab.\n",
    "This practice project provides hands-on experience with data transformation and integration using PySpark. You've performed various tasks, including adding columns, renaming columns, dropping unnecessary columns, joining dataframes, and writing the results into both a Hive warehouse and an HDFS file system.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors\n",
    "\n",
    "Raghul Ramesh\n",
    "\n",
    "Lavanya T S\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--## Change Log -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--|Date (YYYY-MM-DD)|Version|Changed By|Change Description|\n",
    "|-|-|-|-|\n",
    "|2023-09-01|0.1|Lavanya T S|Initial version|\n",
    "|2023-09-08|0.2|Pornima More|QA pass with edits|-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 align=\"center\"> &#169; IBM Corporation. All rights reserved. <h3/>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "prev_pub_hash": "2db360d4eb5b5a825bffde0f271a8cc779b77ac350186b6be51fbb6b3e7c2c4d"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
