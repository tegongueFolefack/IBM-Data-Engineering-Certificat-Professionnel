{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploitation d'Apache Spark pour la Surveillance des Systèmes CVC dans les Bâtiments Intelligents\n",
    "\n",
    "**Temps estimé : 30 minutes**\n",
    "\n",
    "## Objectifs\n",
    "\n",
    "Après avoir terminé ce laboratoire, vous serez capable de :\n",
    "\n",
    "- Expliquer l’architecture distribuée de Spark dans le cadre de la surveillance des bâtiments intelligents\n",
    "- Simuler des données de capteurs en temps réel pour les systèmes CVC (Chauffage, Ventilation et Climatisation) d’un bâtiment\n",
    "- Exécuter des requêtes SQL pour détecter des conditions environnementales critiques et calculer des moyennes de mesures\n",
    "- Afficher les résultats agrégés sur la console pour obtenir des informations immédiates sur les conditions des pièces\n",
    "\n",
    "## Contexte\n",
    "\n",
    "Smart Building Solutions, Inc. est une entreprise spécialisée dans l’optimisation des systèmes CVC (Chauffage, Ventilation et Climatisation) afin d’améliorer le confort et l’efficacité énergétique dans les bâtiments commerciaux. En surveillant en temps réel les niveaux de température et d'humidité dans différentes pièces, l’entreprise vise à garantir des conditions intérieures optimales et à anticiper d’éventuels problèmes liés aux systèmes CVC.\n",
    "\n",
    "Avec un flux continu de données issues des capteurs, Smart Building Solutions doit traiter et analyser ces données en temps réel pour maintenir la qualité de l’environnement intérieur.\n",
    "\n",
    "## Description du jeu de données\n",
    "\n",
    "Le jeu de données simulé comprend :\n",
    "\n",
    "- **`room_id`** : Identifiant unique de chaque pièce (ex. : R001, R002).\n",
    "- **`temperature`** : Relevé actuel de la température du capteur (en °C).\n",
    "- **`humidity`** : Relevé actuel du niveau d’humidité du capteur (en %).\n",
    "- **`timestamp`** : Heure à laquelle la mesure a été enregistrée (générée automatiquement par Spark).\n",
    "\n",
    "Les données sont générées à un rythme de **5 lignes par seconde**, simulant plusieurs pièces avec diverses conditions environnementales.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenges\n",
    "Monitoring indoor environmental conditions poses several challenges:\n",
    "\n",
    "**High data velocity**: Continuous data from multiple sensors can overwhelm traditional systems.\n",
    "\n",
    "**Need for immediate alerts**: Delays in identifying critical conditions can lead to discomfort or system inefficiencies.\n",
    "\n",
    "**Need for data aggregation and analysis**: Efficiently aggregating and analyzing real-time data for proactive maintenance and optimization is essential.\n",
    "\n",
    "## Apache Spark with structured streaming\n",
    "To address these challenges, Apache Spark is employed for its powerful distributed computing capabilities, enabling real-time data processing and analytics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyspark==3.1.2 -q\n",
    "!pip install findspark -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also use this section to suppress warnings generated by your code:\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# FindSpark simplifies the process of using Apache Spark with Python\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "#import functions/Classes for sparkml\n",
    "\n",
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the Spark session:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/05 12:59:51 WARN Utils: Your hostname, tegongue-Latitude-5580 resolves to a loopback address: 127.0.1.1; using 192.168.2.226 instead (on interface wlp1s0)\n",
      "25/03/05 12:59:51 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/tegongue/anaconda3/lib/python3.12/site-packages/pyspark/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "25/03/05 12:59:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/03/05 12:59:53 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/03/05 12:59:53 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "25/03/05 12:59:53 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "25/03/05 12:59:53 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n",
      "25/03/05 12:59:53 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Smart Building HVAC Monitoring\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulate sensor data:\n",
    "\n",
    "Use Spark’s rate source to generate continuous readings from multiple rooms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr, rand,when\n",
    "\n",
    "# Simulate sensor data with room IDs and readings\n",
    "sensor_data = spark.readStream.format(\"rate\").option(\"rowsPerSecond\", 5).load() \\\n",
    "    .withColumn(\"room_id\", expr(\"CAST(value % 10 AS STRING)\")) \\\n",
    "    .withColumn(\"temperature\", when(expr(\"value % 10 == 0\"), 15)  # Set temperature to 15 for one specific record\n",
    "                .otherwise(20 + rand() * 25)) \\\n",
    "    .withColumn(\"humidity\", expr(\"40 + rand() * 30\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a temporary SQL view:\n",
    "\n",
    "Create temporary SQL view to perform SQL queries on the streaming data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a temporary SQL view for the sensor data\n",
    "sensor_data.createOrReplaceTempView(\"sensor_table\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define SQL queries for aggregation and analysis:\n",
    "\n",
    "* **Critical temperature query**: Detect rooms with critical temperature levels\n",
    "* **Average readings query**: Calculate average readings over a 1-minute window\n",
    "* **Attention needed query**: Identify rooms that need immediate attention based on humidity levels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL Query to detect rooms with critical temperatures\n",
    "critical_temperature_query = \"\"\"\n",
    "    SELECT \n",
    "        room_id, \n",
    "        temperature, \n",
    "        humidity, \n",
    "        timestamp \n",
    "    FROM sensor_table \n",
    "    WHERE temperature < 18 OR temperature > 60\n",
    "\"\"\"\n",
    "\n",
    "# SQL Query to calculate average readings over a 1-minute window\n",
    "average_readings_query = \"\"\"\n",
    "    SELECT \n",
    "        room_id, \n",
    "        AVG(temperature) AS avg_temperature, \n",
    "        AVG(humidity) AS avg_humidity, \n",
    "        window.start AS window_start \n",
    "    FROM sensor_table\n",
    "    GROUP BY room_id, window(timestamp, '1 minute')\n",
    "\"\"\"\n",
    "\n",
    "# SQL Query to find rooms that need immediate attention based on humidity\n",
    "attention_needed_query = \"\"\"\n",
    "    SELECT \n",
    "        room_id, \n",
    "        COUNT(*) AS critical_readings \n",
    "    FROM sensor_table \n",
    "    WHERE humidity < 45 OR humidity > 75\n",
    "    GROUP BY room_id\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute the SQL queries:\n",
    "\n",
    "Execute each SQL query to create streaming DataFrames.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the critical temperature query\n",
    "critical_temperatures_stream = spark.sql(critical_temperature_query)\n",
    "\n",
    "\n",
    "# Execute the average readings query\n",
    "average_readings_stream = spark.sql(average_readings_query)\n",
    "\n",
    "# Execute the attention needed query\n",
    "attention_needed_stream = spark.sql(attention_needed_query)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output the results to the console:\n",
    "\n",
    "Display the results from each query in real-time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/05 13:07:43 WARN StreamingQueryManager: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-8b9f1a5f-59a8-46ed-aa76-8bdd463a2459. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/03/05 13:07:44 WARN StreamingQueryManager: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-f6dffa3b-b2d7-492e-a897-a02d9d349d3e. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/03/05 13:07:44 WARN StreamingQueryManager: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-2cdf72b4-8b76-4fa7-85e5-bd0e48b93b69. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n"
     ]
    }
   ],
   "source": [
    "# Output the results to the console for all queries\n",
    "critical_query = critical_temperatures_stream.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .queryName(\"Critical Temperatures\") \\\n",
    "    .start()\n",
    "\n",
    "average_query = average_readings_stream.writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .queryName(\"Average Readings\") \\\n",
    "    .start()\n",
    "\n",
    "attention_query = attention_needed_stream.writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .queryName(\"Attention Needed\") \\\n",
    "    .start()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keep the streams running:\n",
    "\n",
    "Ensure that the streaming queries continue to run to process incoming data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep the streams running\n",
    "spark.stop()\n",
    "print(\"********Critical Temperature Values*******\")\n",
    "critical_query.awaitTermination()\n",
    "\n",
    "print(\"********Average Readings Values********\")\n",
    "average_query.awaitTermination()\n",
    "print(\"********Attention Needed Values********\")\n",
    "attention_query.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "In this lab, you explored the use of Apache Spark in smart building monitoring, particularly focusing on HVAC (heating, ventilation, and air conditioning) systems. You now understand the Spark's distributed architecture. You also understand how to simulate real-time sensor data for temperature and humidity, execute SQL queries to identify critical environmental conditions, and output aggregated results for immediate insights.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Author(s)\n",
    "\n",
    "Lakshmi Holla\n",
    "\n",
    "## Other Contributors\n",
    "Malika Singla\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "prev_pub_hash": "912b8f33a459431fb54207e38887ed2111ae9e5045cafe95bcb575022417b95b"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
